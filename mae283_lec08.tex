\documentclass[lecture,12pt,]{pcms-l}
\input preamble.tex

% For faster processing, load Matlab syntax for listings
\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0}
\lstloadlanguages{Matlab}%
\lstset{language=Matlab,
        frame=single,
        basicstyle=\small\ttfamily,
        keywordstyle=[1]\color{Blue}\bf,
        keywordstyle=[2]\color{Purple},
        keywordstyle=[3]\color{Blue}\underbar,
        identifierstyle=,
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small,
        stringstyle=\color{Purple},
        showstringspaces=false,
        tabsize=5,
        % Put standard MATLAB functions not included in the default
        % language here
        morekeywords={xlim,ylim,var,alpha,factorial,poissrnd,normpdf,normcdf},
        % Put MATLAB function parameters here
        morekeywords=[2]{on, off, interp},
        % Put user defined functions here
        morekeywords=[3]{FindESS},
        morecomment=[l][\color{Blue}]{...},
        numbers=left,
        firstnumber=1,
        numberstyle=\tiny\color{Blue},
        stepnumber=0
        }

% Only the next five fields need to be edited.
\newcommand{\lecAuth}{R.A. de Callafon}
\newcommand{\scribe}{Thomas Denewiler}
\newcommand{\authEmail}{callafon@ucsd.edu}
\newcommand{\scribeEmail}{tdenewiler@gmail.com}
\newcommand{\course}{MAE 283: Parameter Estimation}
\newcommand{\lectureNum}{8}

\address{Department of Mechanical and Aerospace Engineering, University of California, San Diego}

% Adds a hyperlink to an email address.
\newcommand{\mailto}[2]{\href{mailto:#1}{#2}}

% These commands set the document properties for the PDF output. Needs the hyperref package.
\hypersetup
{
    colorlinks,
    linkcolor={black},
    citecolor={black},
    filecolor={black},
    urlcolor={black},
    pdfauthor={\scribe <\mailto{\scribeEmail}{\scribeEmail}>},
    pdfsubject={\course},
    pdftitle={Lecture \lectureNum},
    pdfkeywords={UC San Diego, Parameter Estimation, System Identification},
    pdfstartpage={1},
}

% Includes a figure
% The first parameter is the label, which is also the name of the figure
%   with or without the extension (e.g., .eps, .fig, .png, .gif, etc.)
%   IF NO EXTENSION IS GIVEN, LaTeX will look for the most appropriate one.
%   This means that if a DVI (or PS) is being produced, it will look for
%   an eps. If a PDF is being produced, it will look for nearly anything
%   else (gif, jpg, png, et cetera). Because of this, when I generate figures
%   I typically generate an eps and a png to allow me the most flexibility
%   when rendering my document.
% The second parameter is the width of the figure normalized to column width
%   (e.g. 0.5 for half a column, 0.75 for 75% of the column)
% The third parameter is the caption.
\newcommand{\scalefig}[3]{
  \begin{figure}[ht!]
    % Requires \usepackage{graphicx}
    \centering
	\fbox{
	    \includegraphics[width=#2\columnwidth]{#1}
	}
    %%% I think \captionwidth (see above) can go away as long as
    %%% \centering is above
    %\captionwidth{#2\columnwidth}%
    \caption{#3}
    \label{#1}
  \end{figure}}

% Includes a MATLAB script.
% The first parameter is the label, which also is the name of the script
%   without the .m.
% The second parameter is the optional caption.
\newcommand{\matlabscript}[2]
  {\begin{itemize}\item[]\lstinputlisting[caption=#2,label=#1]{#1.m}\end{itemize}}

% Example environment.
\newtheoremstyle{example}{\topsep}{\topsep}	%
     {}%         Body font
     {}%         Indent amount (empty = no indent, \parindent = para indent)
     {\bfseries}% Thm head font
     {}%        Punctuation after thm head
     {\newline}%     Space after thm head (\newline = linebreak)
     {\thmname{#1}\thmnumber{ #2}\thmnote{ #3}}%         Thm head spec

   \theoremstyle{example}
   \newtheorem{example}{Example}[section]

% A command to show a vector norm that will have the pipe signs scale with the contents.
\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|}

% Commands for time and frequency integrals over infinty, cos and sin.
\newcommand{\tint}{\int_{t=-\infty}^\infty}
\newcommand{\fint}{\int_{\omega=-\infty}^\infty}
\newcommand{\tauint}{\int_{\tau=0}^\infty}
\newcommand{\w}{\omega}
\newcommand{\wo}{\omega_0}
\newcommand{\ejwt}{e^{j\omega t}}
\newcommand{\emjwt}{e^{-j\omega t}}
\newcommand{\dt}{\Delta T}
\newcommand{\tausum}{\sum_{\tau=-\infty}^\infty}
\newcommand{\ksum}{\sum_{k=-\infty}^\infty}
\newcommand{\ruhat}{\hat{R}_u^N(\tau)}
\newcommand{\ryuhat}{\hat{R}_{yu}^N(\tau)}
\newcommand{\phiuhat}{\hat{\phi}_u^N(\omega)}
\newcommand{\phiyuhat}{\hat{\phi}_{yu}^N(\omega)}
\newcommand{\phiyuhh}{\hat{\hat{\phi}}_{yu}^N(\omega)}
\newcommand{\phiuhh}{\hat{\hat{\phi}}_u^N(\omega)}
\newcommand{\phiyhh}{\hat{\hat{\phi}}_y^N(\omega)}
\newcommand{\ghh}{\hat{\hat{G}}(e^{j\omega})}
\newcommand{\thn}{\hat{\theta}_{LS}^N}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\mainmatter
\setcounter{page}{1}

\lectureseries[\course]{\course}

\auth[R.A. de Callafon]{Lecturer: \lecAuth\\ Scribe: \scribe}
\date{October 20, 2009}

\setaddress

% the following hack starts the lecture numbering at 8
\setcounter{lecture}{7}
\setcounter{chapter}{7}

\lecture{Properties of Least Squares Estimates}

\section{Least Squares/Linear Regression Review}
These notes correspond to Chapter 7.3 in Ljung. The least squares estimate is our first attempt to generate a parametric model. The system and model are given by
\begin{align*}
\mathcal{S}: y(t) &= \phi(t)\theta_0 \\
\mathcal{M}: y(t) &= \phi(t)\theta + e(t,\theta)
\end{align*}
where $\phi(t)$ is known as the ``regressor'' and is the same for the model and the system. Also, $\theta_0$ is the ``real'' parameter and $\theta$ is the model of the parameter. Then the least squares estimate of the parameter, $\theta$, is
\begin{align*}
\hat{\theta}_{LS} &= \min_\theta ||e(t,\theta)|| \\
\thn &= \min_\theta \frac{1}{N}\sum_{t=1}^Ne^2(t,\theta) = \min_\theta R_e(\tau=0) = \min_\theta\hat{E}\{e(t,\theta)\}
\end{align*}
where $\hat{E}\{e(t,\theta)\}$ is the estimation of the expected variance. Recall that the regressor, $\phi(t)$, is all of the past inputs and outputs such that
\begin{align}
\label{eq:regressor}
\phi(t) = \left[\begin{array}{c} u(t) \\ u(t-1) \\ \vdots \\ u(t-n_b) \\ -y(t-1) \\ \vdots \\ -y(t-n_a) \end{array}\right]
\end{align}
and the parameter contains
\begin{align}
\label{eq:parameter}
\theta = \left[\begin{array}{c} b_0 \\ b_1 \\ \vdots \\ b_{n_b} \\ a_1 \\ \vdots \\ a_{n_a} \end{array}\right]
\end{align}
The model given by
$$y(t) = \phi(t)\theta + e(t,\theta)$$
can be accumulated over $N$ samples and written in matrix form as
\begin{align*}
\underbrace{\left[\begin{array}{c} y(1) \\ y(2) \\ \vdots \\ y(N) \end{array}\right]}_{\mathbf{y}} &= \underbrace{\left[\begin{array}{c} \phi^T(1) \\ \phi^T(2) \\ \vdots \\ \phi^T(N) \end{array}\right]}_{\mathbf{\phi}^T} \theta
+ \underbrace{\left[\begin{array}{c} e(1,0) \\ e(2,0) \\ \vdots \\ e(N,0) \end{array}\right]}_{\mathbf{e}} \\
\Rightarrow \frac{1}{N}\mathbf{\phi}\mathbf{y} &= \frac{1}{N}\mathbf{\phi}\mathbf{\phi}^T\theta + \frac{1}{N}\mathbf{\phi}\mathbf{e} \\
\Rightarrow \hat{\theta} &= \min_\theta \frac{1}{N}\mathbf{e}^T\mathbf{e}
\end{align*}
This uses an orthogonal projection to make $\hat{\theta}$ as small as possible. Rearranging to get the parameter estimate yields
$$\thn = (\frac{1}{N}\mathbf{\phi\phi}^T)^{-1} (\frac{1}{N}\mathbf{\phi y})$$
Note that this is an ordinal equation and is not the best way to compute the least square estimate from a numerical methods standpoint.

\section{Applications}
\subsection{ARX Model}
The least squares estimate from the perspective of modeling discrete-time systems can be used when the model is given by
\begin{align}
\label{eq:model}
\mathcal{M}: y(t) = \phi(t)\theta + e(t,\theta)
\end{align}
using (\ref{eq:regressor}) and (\ref{eq:parameter}). This is equivalent to writing
\begin{align*}
&y(t) = b_0u(t) + b_1u(t-1) + \cdots + b_{n_b}u(t-n_b) \\
&- a_1y(t-1) - a_2y(t-2) - \cdots - a_{n_a}y(t-n_a) + e(t,\theta) \\
&(1+a_1q^{-1}+a_2q^{-2}+\cdots+a_{n_a}q^{-n_a})y(t) = (b_0+b_1q^{-1}+b_2q^{-2}+\cdots+b_{n_b}q^{-n_b})u(t) + e(t,\theta)
\end{align*}
\begin{align*}
y(t) &= G(q,\theta)u(t) + H(q,\theta)e(t,\theta) \\
&= \frac{b_0+b_1q^{-1}+b_2q^{-2}+\cdots+b_{n_b}q^{-n_b}}{1+a_1q^{-1}+a_2q^{-2}+\cdots+a_{n_a}q^{-n_a}}u(t)
+ \frac{1}{1+a_1q^{-1}+a_2q^{-2}+\cdots+a_{n_a}q^{-n_a}}e(t,\theta)
\end{align*}
Here we can see that the denominators for $G(q,\theta)$ and $H(q,\theta)$ are the same. This is referred to as a ``Autogenous Regression with eXogenous input'' (ARX) model. The ARX model has shared poles between $G(q,\theta)$ and $H(q,\theta)$ and the numerator of $H(q,\theta)=1$.

\subsection{Linear Regression and Noise}
If we look at a system with white noise such as
$$\mathcal{S}: y(t) = \phi^T(t)\theta_0 + e(t)$$
where $\{e(t)\}=$ white noise and the parameter $\theta_0$ is unknown we can re-write this as
$$y(t) = G_0(q)u(t) + v(t)$$
where $v(t)=H_0(q)e(t)$. How do we find the transfer functions of this system? First, note that $G(q,\theta_0)=G_0(q)$. Next we need to find the regressor, $\phi(t)$. We can assume that the regressor is of the form (\ref{eq:regressor}) and contains all past inputs and outputs. Then we know
\begin{align}
\label{eq:g}
G_0(q) = \frac{b_0+b_1q^{-1}+b_2q^{-2}+\cdots+b_{n_b}q^{-n_b}}{1+a_1q^{-1}+a_2q^{-2}+\cdots+a_{n_a}q^{-n_a}}
\end{align}
and
\begin{align}
\label{eq:h}
H_0(q) = \frac{1}{1+a_1q^{-1}+a_2q^{-2}+\cdots+a_{n_a}q^{-n_a}}
\end{align}
This means that we are implicitly assuming that the noise on the system is filtered such that it has the exact same poles as the real data. That is a very restrictive assumption as it does not apply to many real-world systems.

If we use $\mathcal{S}: y(t) = G_0(q)u(t) = e(t)$ where $\{e(t)\}$ is white noise then we cannot write it as a linear regression because the noise and the system transfer functions do not have the same poles. An exception is when we are looking at a Finite Impulse Response (FIR) system. Then the denominators for both transfer functions can be $1$ and the current output has no dependance on past outputs.

\subsection{State Space}
Given a state space model
\begin{align*}
\mathcal{M}: \begin{cases} x(t+1)&=Ax(t)+Bu(t)+v(t) \\ y(t)&=Cx(t)+Du(t)+w(t) \end{cases}
\end{align*}
where $\{v(t)\}$ and $\{w(t)\}$ are white noise and assuming that we have access to/measurements of $\{u(t),y(t)\} + \{x(t)\}$ then the estimates of the system matrices can be written as
\begin{align}
\left[\begin{array}{c} x(t+1) \\ y(t) \end{array}\right] &=
\left[\begin{array}{c c} A & B \\ C & D \end{array}\right] \left[\begin{array}{c} x(t) \\ u(t) \end{array}\right] +
\left[\begin{array}{c} v(t) \\ w(t) \end{array}\right] \nonumber \\
\bar{y}(t) &= \bar{\theta}^T\bar{phi}(t) + \bar{e}(t) \nonumber \\
\bar{y}^T(t) &= \bar{\phi}^T(t)\bar{\theta} + \bar{e}^T(t)
\end{align}
Conclusion: If $\{u(t),y(t)\} + \{x(t)\}$ is available then the estimation of $A,B,C,D$ can be written as a least squares problem.

\section{Properties of Least Squares Estimate}
Some important properties of the least squares estimate, $\thn$, are given below.

\subsection{Relation to the Covariance Functions}
$$\thn = (\frac{1}{N}\mathbf{\phi\phi}^T)^{-1}(\frac{1}{N}\mathbf{\phi y}) \triangleq \underbrace{R(N)^{-1}}_{p\times p} \underbrace{f(N)}_{p\times 1}$$
Looking more closely at $R(N)$ we can see that it is a $p\times p$ matrix built from elements of (\ref{eq:regressor}).
$$R(N) = \frac{1}{N}\left[\begin{array}{c c c} \phi(1) & \cdots & \phi(N) \end{array}\right]
\left[\begin{array}{c} \phi^T(1) \\ \vdots \\ \phi^T(N) \end{array}\right]$$
Calculating a few of the elements of $R(N)$ gives
\begin{align*}
R(N)_{1,1} &= \frac{1}{N}\sum_{t=1}^Nu(t)u(t) = \hat{R}_u^N(0) \\
R(N)_{1,2} &= \frac{1}{N}\sum_{t=1}^Nu(t)u(t-1) = \hat{R}_u^N(1) \\
R(N)_{2,1} &= \frac{1}{N}\sum_{t=1}^Nu(t-1)u(t) = \hat{R}_u^N(1)
\end{align*}
The last two equalities show that $R(N)$ has some symmetry within it. The general form for $R(N)$ is
\begin{align*}
\left[\begin{array}{c c}
% Top left.
\begin{array}{c c c c|}
\hat{R}_u^N(0) & \hat{R}_u^N(1) & \cdots & \hat{R}_u^N(n_b) \\
\hat{R}_u^N(1) & \hat{R}_u^N(0) & \cdots & \hat{R}_u^N(n_b-1) \\
\vdots & \vdots & \ddots & \vdots \\
\hat{R}_u^N(n_b) & \hat{R}_u^N(n_b-1) & \cdots & \hat{R}_u^N(0) \\
\hline
\end{array}
% Top right.
\begin{array}{c c c}
\hat{R}_{yu}^N(1) & \cdots & \hat{R}_{yu}^N(n_a) \\
\hat{R}_{yu}^N(0) & \cdots & \hat{R}_{yu}^N(n_a-1) \\
\vdots & \ddots & \vdots \\
\hat{R}_{yu}^N(n_a-n_b-1) & \cdots & \hat{R}_{yu}^N(0) \\
\hline
\end{array} \\
% Bottom left.
\begin{array}{c c c|}
\hat{R}_{yu}^N(1) & \cdots & \hat{R}_{yu}^N(n_a) \\
\hat{R}_{yu}^N(0) & \cdots & \hat{R}_{yu}^N(n_a-1) \\
\vdots & \ddots & \vdots \\
\hat{R}_{yu}^N(n_a-n_b-1) & \cdots & \hat{R}_{yu}^N(0) \\
\end{array}
% Bottom right.
\begin{array}{c c c c}
\hat{R}_y^N(0) & \hat{R}_y^N(1) & \cdots & \hat{R}_y^N(n_b) \\
\hat{R}_y^N(1) & \hat{R}_y^N(0) & \cdots & \hat{R}_y^N(n_b-1) \\
\vdots & \vdots & \ddots & \vdots \\
\hat{R}_y^N(n_b) & \hat{R}_y^N(n_b-1) & \cdots & \hat{R}_y^N(0)
\end{array}
\end{array}\right]
\end{align*}
In the block matrix above the off-diagonal elements are supposed to be tranposes of each other but it's not shown that way currently. The $[1,1]$ and $[2,2]$ blocks are Toeplitz and $[1,2]$ and $[2,1]$ are transposes of each other. This can be written as
$$R(N) = \left[\begin{array}{c c c} \mathbf{R}_u & \vline & \mathbf{R}_{yu} \\
\hline \\
\mathbf{R}_{yu}^T & \vline & \mathbf{R}_y \end{array}\right]$$

We also have that
\begin{align*}
f(N) = \left[\begin{array}{c}
\hat{R}_{yu}^N(0) \\
\hat{R}_{yu}^N(1) \\
\vdots \\
\hat{R}_{yu}^N(n_a-n_b-1) \\
\hline
\hat{R}_y^N(1) \\
\vdots \\
\hat{R}_y^N(n_a) \end{array}\right]
\end{align*}
Conclusion: the least squares estimate for the parameter is filled with auto-/cross-covariance function estimates.

\subsection{Invertibility of $R(N)$}
For an FIR model we only have the $\mathbf{R}_u$ block as all the rest of the terms in $R(N)=0$. Then, for FIR, $\mathbf{R}_u$ must have $\{u(t)\}$ be a persistently exciting signal to ensure the invertibility of $\mathbf{R}_u$.

When we are not looking at an FIR model we know that $\mathbf{R}_u$ is invertible and must use the matrix inversion lemma to determine if $R(N)$ is invertible.
\begin{definition}
The matrix inversion lemma states that
$$\det\left(\left[\begin{array}{c c}A&B\\C&D\end{array}\right]\right) = \det(A) \cdot \det(D-CA^{-1}B)$$
\end{definition}
For $R(N)$ to be invertible it must be true that $\det(\mathbf{R}_y-\mathbf{R}_{yu}^T\mathbf{R}_u^{-1}\mathbf{R}_{yu})\neq 0$. Note that $\mathbf{R}_y$ is usually perturbed by noise. However, we stil need to have $\{u(t)\}$ be a persistently exciting signal.

\subsection{Consistency of the Parameter Estimation}
What happens in
$$\lim_{N\to\infty} \thn  = \theta_0 \text{ w.p. } 1$$
Is this a true statement? If we use the model
$$\mathcal{M}: y(t) = \phi^T(t)\theta + e(t,\theta)$$
and assume that the system is
$$\mathcal{S}: y(t) = \phi^T(t)\theta_0 + v(t)$$
where the assumption means that $\theta$ and $\theta_0$ have the same size, $\{v(t)\}$ is not necessarily white noise, $E\{v(t)\} = 0$ and $u\perp v$, then
\begin{align}
\label{eq:thn}
\thn &= R(N)^{-1}f(N) \nonumber \\
&= R(N)^{-1}\left(\frac{1}{N}\sum_{t=1}^N\phi(t)y(t)\right) \nonumber \\
&= R(N)^{-1}\left( \underbrace{\frac{1}{N}\sum_{t=1}^N\phi(t)\phi^T(t)}_{R(N)}\theta_0+\frac{1}{N}\sum_{t=1}^N\phi(t)v(t)\right) \nonumber\\
&= \theta_0 + R(N)^{-1}\cdot \frac{1}{N}\sum_{t=1}^N\phi(t)v(t)
\end{align}
This leads to
$$\lim_{N\to\infty} E\{\thn\} = \theta_0 + \lim_{N\to\infty}R(N)^{-1}\frac{1}{N}\sum_{t=1}^N\phi(t)v(t)$$
We want the last term to go to zero so that
$$\lim_{N\to\infty} E\{\thn\} = 0$$
That happens when
$$\lim_{N\to\infty} R(N)^{-1}R_{\phi v}(0) = 0$$
Recall that $\phi(t)$ is given by (\ref{eq:regressor}) and $u\perp v$. Then we have that
$$R_{\phi v}(0) = \left[\begin{array}{c} R_{uv}(0) \\ R_{uv}(-1) \\ \vdots \\ R_{uv}(-n_b) \\ \hline \\ R_{yv}(-1) \\ \vdots \\ R_{yv}(-n_a) \end{array}\right]$$
The first $n_b$ terms in the upper block are zero because of $u\perp v$. The last $n_a$ terms in the lower block are zero if and only if $\{v(t)\}$ is white noise.

Conclusion: For $\mathcal{S}: y(t) = \phi^T(t)\theta_0 + v(t)$ it must be true that $\{v(t)\}$ is white noise in order to avoid bias in the least squares estimation of the parameter.

\subsection{Careful with the White Noise Assumption}
Given a system and model such that
\begin{align*}
\mathcal{S}: y(t) &= G_0(q)u(t) + e(t) \\
\mathcal{M}: y(t) &+ \phi^T(t)\theta + e(t,\theta)
\end{align*}
where $\{e(t)\}$ is white noise we get
$$\thn = \min_\theta \frac{1}{N}\sum_{t=1}^N e^2(t,\theta)$$
What happens when
$$\lim_{N\to\infty} E\{\thn\}$$
Does it go to the real parameter, $\theta_0$?

We can re-write this as
\begin{align*}
y(t) &= \frac{B(q)}{A(q}u(t) + e(t) \\
y(t)A(q) &= B(q)u(t) + e(t) \\
y(t) &= \phi^T(t)\theta_0 + v(t) \\
v(t) &= A(q)e(t)
\end{align*}
The last equality shows that the noise is actually colored by the filter, $A(q)$ and so $\{v(t)\}$ is \textit{not} white noise. The noise we actually measure is $\{v(t)\}$ and not $\{e(t)\}$. The only time that we can say that we have white noise in this situation is when
$$\mathcal{S}: y(t) = G_0(q)u(t) + H_0(q)e(t)$$
with
$$H_0(q) = \frac{1}{A(q)}$$


\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%